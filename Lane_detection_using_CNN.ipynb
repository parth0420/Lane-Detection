{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lane detection using CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5gghy7_HEod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8faa867-313c-469e-b676-3f43e6efb3bc"
      },
      "source": [
        "#import necessary libraries\n",
        "import numpy as np                       \n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, UpSampling2D\n",
        "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import HTML\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b876544/45929032 bytes (1.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2383872/45929032 bytes (5.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4243456/45929032 bytes (9.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6701056/45929032 bytes (14.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9314304/45929032 bytes (20.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11976704/45929032 bytes (26.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14311424/45929032 bytes (31.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16883712/45929032 bytes (36.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19488768/45929032 bytes (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21987328/45929032 bytes (47.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24625152/45929032 bytes (53.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27344896/45929032 bytes (59.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29958144/45929032 bytes (65.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32604160/45929032 bytes (71.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35364864/45929032 bytes (77.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37994496/45929032 bytes (82.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40591360/45929032 bytes (88.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43261952/45929032 bytes (94.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45834240/45929032 bytes (99.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXG9k9OiRXtn"
      },
      "source": [
        "train_images = pickle.load(open(\"full_CNN_train.p\", \"rb\" ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq-lKDwFfKg1"
      },
      "source": [
        "train_images= train_images[0:5]  #slicing to take less images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVXfsakwgGNz"
      },
      "source": [
        "labels = pickle.load(open(\"full_CNN_labels.p\", \"rb\" ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cWGZAxof9wa"
      },
      "source": [
        "labels=labels[0:5]    #slicing to take less labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVHm1tcdHJoL"
      },
      "source": [
        "def create_model(input_shape, pool_size):\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization(input_shape=input_shape))\n",
        "  model.add(Conv2D(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))    #conv layer1 \n",
        "  model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 2\n",
        "  model.add(MaxPooling2D(pool_size=pool_size))                                         #pooling layer 1\n",
        "  model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 3\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 4\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 5\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling2D(pool_size=pool_size))                                         #pooling layer 2\n",
        "  model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 6\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #conv layer 7\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling2D(pool_size=pool_size))                                         #pooling layer 3\n",
        "  model.add(UpSampling2D(size=pool_size))                                              #upsample layer 1\n",
        "  model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')) #deconv layer 1\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv2'))  #deconv layer 2\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(UpSampling2D(size=pool_size))                                               #upsample layer 2\n",
        "  model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')) #deconv layer 3\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))  #deconv layer 4\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))  #deconv layer 5\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(UpSampling2D(size=pool_size))                                                      #upsample layer 3\n",
        "  model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))  #deconv layer 6\n",
        "  model.add(Conv2DTranspose(1, (3, 3), padding='valid', strides=(1,1), activation = 'relu'))   #final layer including one channel only\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHer5YIULRRn",
        "outputId": "af2019d0-729b-4dc7-e2d5-43f48b48f5fd"
      },
      "source": [
        "def main():\n",
        "  train_images = pickle.load(open(\"full_CNN_train.p\", \"rb\" ))  #loading training images\n",
        "  labels = pickle.load(open(\"full_CNN_labels.p\", \"rb\" ))       #loading image labels\n",
        "  train_images = np.array(train_images)   #making arrays for neural network\n",
        "  labels = np.array(labels)\n",
        "  labels = labels / 255     #normalizing labels\n",
        "  train_images, labels = shuffle(train_images, labels)  #shuffling images along with its labels\n",
        "  X_train, X_val, y_train, y_val = train_test_split(train_images, labels, test_size=0.1)  #splitting in training and validation sets\n",
        "  batch_size = 32   #optimizing\n",
        "  epochs = 10\n",
        "  pool_size = (2, 2)\n",
        "  input_shape = X_train.shape[1:]\n",
        "  model = create_model(input_shape, pool_size)    #creating the model\n",
        "  datagen = ImageDataGenerator(channel_shift_range=0.2)   #generator to help model use less data\n",
        "  datagen.fit(X_train)\n",
        "  model.compile(optimizer='Adam', loss='mean_squared_error')   #compiling and training the model\n",
        "  model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,\n",
        "  epochs=epochs, verbose=1, validation_data=(X_val, y_val))\n",
        "  model.trainable = False       #freezing layer as training is done\n",
        "  model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "  model.save('full_CNN_model.h5')  #saving the model\n",
        "  model.summary()       #summarzing the model\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "358/358 [==============================] - 752s 2s/step - loss: 0.0517 - val_loss: 0.0098\n",
            "Epoch 2/10\n",
            "358/358 [==============================] - 775s 2s/step - loss: 0.0101 - val_loss: 0.0082\n",
            "Epoch 3/10\n",
            "358/358 [==============================] - 771s 2s/step - loss: 0.0086 - val_loss: 0.0075\n",
            "Epoch 4/10\n",
            "358/358 [==============================] - 768s 2s/step - loss: 0.0081 - val_loss: 0.0073\n",
            "Epoch 5/10\n",
            "358/358 [==============================] - 768s 2s/step - loss: 0.0074 - val_loss: 0.0055\n",
            "Epoch 6/10\n",
            "358/358 [==============================] - 764s 2s/step - loss: 0.0061 - val_loss: 0.0052\n",
            "Epoch 7/10\n",
            "358/358 [==============================] - 764s 2s/step - loss: 0.0057 - val_loss: 0.0051\n",
            "Epoch 8/10\n",
            "358/358 [==============================] - 769s 2s/step - loss: 0.0056 - val_loss: 0.0050\n",
            "Epoch 9/10\n",
            "358/358 [==============================] - 767s 2s/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 10/10\n",
            "358/358 [==============================] - 764s 2s/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 80, 160, 3)        12        \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 78, 158, 8)        224       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 76, 156, 16)       1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 38, 78, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 36, 76, 16)        2320      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 36, 76, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 34, 74, 32)        4640      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 34, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 72, 32)        9248      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 34, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 14, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 12, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 12, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 16, 64)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 12, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 14, 34, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 14, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "Deconv2 (Conv2DTranspose)    (None, 16, 36, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 32, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 34, 74, 32)        18464     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 34, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 36, 76, 32)        9248      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 36, 76, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 38, 78, 16)        4624      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 38, 78, 16)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 76, 156, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 78, 158, 16)       2320      \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 80, 160, 1)        145       \n",
            "=================================================================\n",
            "Total params: 181,693\n",
            "Trainable params: 0\n",
            "Non-trainable params: 181,693\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H76kJcaj5ld"
      },
      "source": [
        "class Lanes():\n",
        "    def __init__(self):\n",
        "        self.recent_fit = []\n",
        "        self.avg_fit = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm4W3ZGSlEVV"
      },
      "source": [
        "from skimage.transform import resize\n",
        "def road_lines(image):\n",
        "    \"\"\" Takes in a road image, re-sizes for the model,\n",
        "    predicts the lane to be drawn from the model in G color,\n",
        "    recreates an RGB image of a lane and merges with the\n",
        "    original road image.\n",
        "    \"\"\"\n",
        "\n",
        "    # feeding image in model\n",
        "    small_img = resize(image, (80, 160, 3))\n",
        "    small_img = np.array(small_img)\n",
        "    small_img = small_img[None,:,:,:]\n",
        "\n",
        "    # Making prediction with neural network \n",
        "    prediction = model.predict(small_img)[0] * 255  #un-normalize value by multiplying by 255\n",
        "\n",
        "    # Adding lane prediction \n",
        "    lanes.recent_fit.append(prediction)\n",
        "    # Only using last three for average\n",
        "    if len(lanes.recent_fit) > 3:\n",
        "        lanes.recent_fit = lanes.recent_fit[1:]\n",
        "\n",
        "    # Calculate average detection\n",
        "    lanes.avg_fit = np.mean(np.array([i for i in lanes.recent_fit]), axis = 0)\n",
        "\n",
        "    # Generate fake R & B color dimensions, stacking with G\n",
        "    blanks = np.zeros_like(lanes.avg_fit).astype(np.uint8)\n",
        "    lane_drawn = np.dstack((blanks, lanes.avg_fit, blanks))\n",
        "\n",
        "    # Re-size to match the original image\n",
        "    lane_image = resize(lane_drawn, (720, 1280, 3))\n",
        "\n",
        "    # Merge the lane drawing onto the original image\n",
        "    result = cv2.addWeighted(image, 1, lane_image, 1, 0)\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "1WAvXaMHlJnL",
        "outputId": "29a13097-c980-4c72-801f-237efd1bebc2"
      },
      "source": [
        "import tensorflow as tf\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    model = tf.keras.models.load_model('full_CNN_model.h5')\n",
        "    # Create lanes object\n",
        "    lanes = Lanes()\n",
        "\n",
        "    # saving output video\n",
        "    vid_output = 'output_vid.mp4'\n",
        "    # input video\n",
        "    clip1 = VideoFileClip(\"input_video.mp4\")\n",
        "    # Create the clip\n",
        "    vid_clip = clip1.fl_image(road_lines)\n",
        "    vid_clip.write_videofile(vid_output, audio=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9b88863cddd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_video.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Create the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mvid_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvid_clip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mfl_image\u001b[0;34m(self, image_func, apply_to)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mfl\u001b[0;34m(self, fun, apply_to, keep_duration)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-171>\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36moutplace\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\" Applies f(clip.copy(), *a, **k) and returns clip.copy()\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \"\"\"\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moutplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-04374df004e4>\u001b[0m in \u001b[0;36mroad_lines\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Merge the lane drawing onto the original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlane_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/core/src/arithm.cpp:687: error: (-5:Bad argument) When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified in function 'arithm_op'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpCv6WnIJIny"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}